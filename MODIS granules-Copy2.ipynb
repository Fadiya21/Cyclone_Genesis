{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d93a641c-2733-4e9f-a63b-4e7f8f2e93f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f2/_6r3zjz171xcs8vgzgkdppc00000gn/T/ipykernel_85106/717112647.py:96: UserWarning: Warning: converting a masked element to nan.\n",
      "  \"MCS_minBT\": float(ctt[mask].min()),\n",
      "/var/folders/f2/_6r3zjz171xcs8vgzgkdppc00000gn/T/ipykernel_85106/717112647.py:97: UserWarning: Warning: converting a masked element to nan.\n",
      "  \"MCS_avgBT\": float(ctt[mask].mean()),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 115 rows to mcs_full.csv\n",
      "                 time                                       granule  \\\n",
      "0 2002-09-01 00:45:00  MYD06_L2.A2002244.0045.061.2018004074116.hdf   \n",
      "1 2002-09-01 00:45:00  MYD06_L2.A2002244.0045.061.2018004074116.hdf   \n",
      "2 2002-09-01 00:45:00  MYD06_L2.A2002244.0045.061.2018004074116.hdf   \n",
      "3 2002-09-01 00:45:00  MYD06_L2.A2002244.0045.061.2018004074116.hdf   \n",
      "4 2002-09-01 00:45:00  MYD06_L2.A2002244.0045.061.2018004074116.hdf   \n",
      "\n",
      "          lon        lat   MCS_minBT   MCS_avgBT  MCS_size  \n",
      "0 -166.064102  10.966345         NaN         NaN       670  \n",
      "1 -172.369415  10.166039         NaN         NaN      1263  \n",
      "2 -178.838989  11.009789  232.129995  232.129995      1042  \n",
      "3 -165.777390  16.864054  198.789996  222.299237      3341  \n",
      "4 -178.480469  20.104568  194.589996  217.285998     13885  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyhdf.SD import SD, SDC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from scipy.ndimage import label\n",
    "\n",
    "# Constants\n",
    "THRESH_BT = 233  # Brightness temperature threshold (K)\n",
    "MIN_PIXELS = 600  # Minimum number of pixels to classify as MCS\n",
    "\n",
    "# Function to read MODIS HDF files and extract relevant datasets (latitude, longitude, temperature)\n",
    "def read_modis_hdf(hdf_files):\n",
    "    modis_data = []\n",
    "    \n",
    "    for file in hdf_files:\n",
    "        try:\n",
    "            hdf = SD(file, SDC.READ)  # Open the HDF file\n",
    "            \n",
    "            # Extract relevant datasets\n",
    "            latitude = hdf.select('Latitude')[:]\n",
    "            longitude = hdf.select('Longitude')[:]\n",
    "            brightness_temperature = hdf.select('Brightness_Temperature')[:]\n",
    "            scan_start_time = hdf.select('Scan_Start_Time')[:]  # Assuming this field exists for time\n",
    "            \n",
    "            # Convert the time from seconds since 1993-01-01 to datetime\n",
    "            base_time = datetime(1993, 1, 1)\n",
    "            time = [base_time + timedelta(seconds=t) for t in scan_start_time.flatten()]\n",
    "            \n",
    "            # Store the extracted data\n",
    "            modis_data.append({\n",
    "                'latitude': latitude,\n",
    "                'longitude': longitude,\n",
    "                'brightness_temperature': brightness_temperature,\n",
    "                'time': time  # Add time to the data\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "    \n",
    "    return modis_data\n",
    "\n",
    "# Function to process granules (read files, extract data, and write CSV)\n",
    "def process_granules(granules, start_date, end_date, output_csv):\n",
    "    records = []\n",
    "    \n",
    "    for f in granules:\n",
    "        fn = Path(f).name\n",
    "        parts = fn.split(\".\")\n",
    "        year = int(parts[1][1:5])\n",
    "        doy  = int(parts[1][5:])\n",
    "        hhmm = parts[2]\n",
    "        hour   = int(hhmm[:2])\n",
    "        minute = int(hhmm[2:])\n",
    "        \n",
    "        # Build a datetime object based on year, day of year, hour, and minute\n",
    "        date0 = datetime(year, 1, 1) + timedelta(days=doy-1, hours=hour, minutes=minute)\n",
    "\n",
    "        try:\n",
    "            # Open the granule\n",
    "            sd = SD(f, SDC.READ)\n",
    "            ctt_raw = sd.select('Cloud_Top_Temperature')[:].astype(float)\n",
    "            lat = sd.select('Latitude')[:]\n",
    "            lon = sd.select('Longitude')[:]\n",
    "            attrs = sd.select('Cloud_Top_Temperature').attributes()\n",
    "            ctt = (ctt_raw - attrs['add_offset']) * attrs['scale_factor']\n",
    "            ctt = np.ma.masked_where(ctt_raw == attrs['_FillValue'], ctt)  # Mask invalid values\n",
    "\n",
    "            # Check if data is empty or invalid\n",
    "            if np.all(np.isnan(ctt)):\n",
    "                print(f\"Skipping granule {f} because data is invalid.\")\n",
    "                continue  # Skip invalid granules\n",
    "\n",
    "            # Wrap longitude to the range [-180, 180]\n",
    "            lon_wrapped = (lon + 180.) % 360. - 180.\n",
    "            cold = ctt <= THRESH_BT\n",
    "            blobs, nlab = label(cold)\n",
    "\n",
    "            for lab in range(1, nlab + 1):\n",
    "                mask = blobs == lab\n",
    "                if mask.sum() < MIN_PIXELS:\n",
    "                    continue  # Skip blobs that don't meet the size requirement\n",
    "\n",
    "                # Calculate average latitude and longitude for the blob\n",
    "                clat = float(lat[mask].mean())\n",
    "                clon = float(np.degrees(np.arctan2(np.sin(np.radians(lon_wrapped[mask])).mean(),\n",
    "                                                   np.cos(np.radians(lon_wrapped[mask])).mean())))\n",
    "                \n",
    "                # Append the detected MCS data to the records list\n",
    "                records.append({\n",
    "                    \"time\": date0,\n",
    "                    \"granule\": fn,\n",
    "                    \"lon\": clon,\n",
    "                    \"lat\": clat,\n",
    "                    \"MCS_minBT\": float(ctt[mask].min()),\n",
    "                    \"MCS_avgBT\": float(ctt[mask].mean()),\n",
    "                    \"MCS_size\": int(mask.sum()),\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing granule {f}: {e}\")\n",
    "    \n",
    "    # Write the records to a CSV file\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Wrote {len(df)} rows to {output_csv}\")\n",
    "    return df\n",
    "\n",
    "# List of available granules (as provided by you)\n",
    "granules = [\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002244.0045.061.2018004074116.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002244.0220.061.2018004075640.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002244.0400.061.2018004075305.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002244.0540.061.2018004074751.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002244.0715.061.2018004075605.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002245.0125.061.2018004080039.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002245.0300.061.2018004081337.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002245.0440.061.2018004075606.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002245.0630.061.2018004075900.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002246.0035.061.2018004081156.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002246.0215.061.2018004082114.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002246.0355.061.2018004081219.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002246.0535.061.2018004082029.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002246.0710.061.2018004081945.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002247.0110.061.2018004083640.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002247.0250.061.2018004083644.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002247.0430.061.2018004082439.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002247.0605.061.2018004082100.hdf',\n",
    "    '/Users/fadiya/Documents/cycone/data/downloads/MYD06_L2.A2002247.0745.061.2018004083222.hdf'\n",
    "]\n",
    "\n",
    "# Define the date range for processing\n",
    "start_date = \"2002-09-01\"\n",
    "end_date = \"2002-09-04\"\n",
    "output_csv = \"mcs_full.csv\"\n",
    "\n",
    "# Process the granules and output the MCS data\n",
    "mcs_df = process_granules(granules, start_date, end_date, output_csv)\n",
    "\n",
    "# Show the first few rows of the processed data\n",
    "print(mcs_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412cbafe-4312-4992-b304-bd02159c9a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
